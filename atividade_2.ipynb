{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8185f7a",
   "metadata": {},
   "source": [
    "# Atividade MC 2 - Implementação de Gradiente Descendente e Redes Neurais\n",
    "\n",
    "Este notebook contém a resolução dos exercícios propostos na \"MC Atividade 2\". O foco é traduzir a lógica visual dos slides (StatQuest) para código Python funcional.\n",
    "\n",
    "Bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Configuração para melhorar a visualização dos gráficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b6868",
   "metadata": {},
   "source": [
    "## Questão A: Gradiente Descendente (Regressão Linear)\n",
    "\n",
    "**Objetivo:** Implementar o procedimento dos slides 156-213.\n",
    "* **Dados:** Altura vs Peso.\n",
    "* **Modelo:** `Altura = Intercepto + 0.64 * Peso` (O declive/slope é fixo em 0.64).\n",
    "* **Tarefa:** Otimizar o **Intercepto** começando de 0.\n",
    "* **Visualização:** Mostrar o movimento da reta e a descida na curva de custo (SSR).\n",
    "* **Condições de parada:** Número máximo de iterações OU Step Size muito pequeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_question_a(learning_rate, max_iter=20, tolerance=0.001):\n",
    "    #dados do slide \n",
    "    weights = np.array([0.5, 2.3, 2.9])\n",
    "    heights = np.array([1.4, 1.9, 3.2])\n",
    "    \n",
    "    #parâmetros Iniciais que foram definidos no slide\n",
    "    slope_fixed = 0.64\n",
    "    intercept = 0.0  # Começa em 0 conforme o slide\n",
    "    \n",
    "    #para plotagem da curva de custo\n",
    "    intercept_vals = np.linspace(-0.5, 1.5, 100)\n",
    "    ssr_vals = []\n",
    "    for i in intercept_vals:\n",
    "        predicted = i + slope_fixed * weights\n",
    "        residuals = heights - predicted\n",
    "        ssr_vals.append(np.sum(residuals**2))\n",
    "        \n",
    "    print(f\"\\n--- Iniciando Gradiente Descendente (LR: {learning_rate}) ---\")\n",
    "    \n",
    "    #loop para otimização do intercepto\n",
    "    for i in range(max_iter):\n",
    "        #calcular Predições e Derivada\n",
    "        predicted_heights = intercept + slope_fixed * weights\n",
    "        \n",
    "        #calculo do predicted e da derivada\n",
    "        residuals = heights - predicted_heights\n",
    "        derivative = np.sum(-2 * residuals)\n",
    "        \n",
    "        # calcular Step Size\n",
    "        step_size = derivative * learning_rate\n",
    "        \n",
    "        #guardar valor antigo para impressão\n",
    "        old_intercept = intercept\n",
    "        \n",
    "        #atualizar intercept\n",
    "        new_intercept = old_intercept - step_size\n",
    "        \n",
    "        print(f\"Iter {i+1}: Old Intercept={old_intercept:.3f}, Step Size={step_size:.3f}, New Intercept={new_intercept:.3f}, Slope(Grad)={derivative:.3f}\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        #grafico da reta de regressão\n",
    "        axes[0].scatter(weights, heights, color='green', s=100, label='Dados Reais')\n",
    "        axes[0].plot(weights, predicted_heights, color='teal', linewidth=2, label=f'Reta (Intercept={old_intercept:.2f})')\n",
    "\n",
    "        for w, h, p in zip(weights, heights, predicted_heights):\n",
    "            axes[0].plot([w, w], [h, p], color='red', linestyle='--')\n",
    "        axes[0].set_title(f\"Iteração {i+1}: Ajuste da Reta\")\n",
    "        axes[0].set_xlabel(\"Peso\")\n",
    "        axes[0].set_ylabel(\"Altura\")\n",
    "        axes[0].legend()\n",
    "        axes[0].set_ylim(0, 4)\n",
    "        \n",
    "        # Gráfico da curva de custo\n",
    "        current_ssr = np.sum((heights - (old_intercept + slope_fixed * weights))**2)\n",
    "        axes[1].plot(intercept_vals, ssr_vals, color='teal')\n",
    "        axes[1].scatter(old_intercept, current_ssr, color='red', s=100, zorder=5)\n",
    "        #tangente (visualização do gradiente)\n",
    "        tangent_line = derivative * (intercept_vals - old_intercept) + current_ssr\n",
    "        axes[1].plot(intercept_vals, tangent_line, color='orange', linestyle='--', alpha=0.5, label='Inclinação (Derivada)')\n",
    "        \n",
    "        axes[1].set_title(f\"Soma dos Resíduos ao Quadrado vs Intercepto\")\n",
    "        axes[1].set_xlabel(\"Intercepto\")\n",
    "        axes[1].set_ylabel(\"SSR\")\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        intercept = new_intercept\n",
    "        \n",
    "        #condição de parada 2: step size muito pequeno\n",
    "        if abs(step_size) < tolerance:\n",
    "            print(\"-> Convergência alcançada (Step Size pequeno).\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Resultado Final para LR {learning_rate}: Intercepto = {intercept:.4f}\")\n",
    "\n",
    "\n",
    "run_question_a(learning_rate=0.1)\n",
    "\n",
    "run_question_a(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b77a2",
   "metadata": {},
   "source": [
    "## Questão B: Gradiente Descendente Estocástico e Mini-Batch\n",
    "\n",
    "**Objetivo:** Transformar o procedimento dos slides 302-335 em código.\n",
    "* **Diferença:** Não atualizar usando a soma de *todos* os dados de uma vez, mas sim amostra por amostra (Estocástico) ou em pequenos grupos (Mini-batch).\n",
    "* **Nota:** O gráfico complexo da Questão A não será gerado aqui, focaremos na convergência."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a04927",
   "metadata": {},
   "source": [
    "### B-1: Gradiente Descendente Estocástico (SGD)\n",
    "Atualiza o intercepto após calcular o erro para **cada** amostra individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d64cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SGD (Learning Rate: 0.1) ---\n",
      "Época 1:\n",
      "   Amostra (w=2.3): Old=0.000, Step=-0.086, New=0.086\n",
      "   Amostra (w=0.5): Old=0.086, Step=-0.199, New=0.284\n",
      "   Amostra (w=2.9): Old=0.284, Step=-0.212, New=0.496\n",
      "Final SGD Intercept= 0.4964\n"
     ]
    }
   ],
   "source": [
    "def run_sgd(learning_rate=0.1, epochs=5):\n",
    "    weights = np.array([0.5, 2.3, 2.9])\n",
    "    heights = np.array([1.4, 1.9, 3.2])\n",
    "    slope_fixed = 0.64\n",
    "    intercept = 0.0\n",
    "    \n",
    "    print(f\"--- SGD (Learning Rate: {learning_rate}) ---\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(len(weights))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        print(f\"Época {epoch+1}:\")\n",
    "        \n",
    "        for i in indices:\n",
    "            w = weights[i]\n",
    "            h = heights[i]\n",
    "            \n",
    "            pred = intercept + slope_fixed * w\n",
    "\n",
    "            derivative = -2 * (h - pred)\n",
    "\n",
    "            step_size = derivative * learning_rate\n",
    "            \n",
    "            old_intercept = intercept\n",
    "            intercept = intercept - step_size\n",
    "            \n",
    "            print(f\"   Amostra (w={w}): Old={old_intercept:.3f}, Step={step_size:.3f}, New={intercept:.3f}\")\n",
    "\n",
    "        return intercept\n",
    "\n",
    "final_intercept_sgd = run_sgd()\n",
    "print(f\"Final SGD Intercept= {final_intercept_sgd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0feff5d",
   "metadata": {},
   "source": [
    "### B-2: Gradiente Descendente Mini-Batch\n",
    "Adaptação para usar **mini-batch de 2 samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42333ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minibatch(learning_rate=0.1, epochs=5, batch_size=2):\n",
    "    weights = np.array([0.5, 2.3, 2.9])\n",
    "    heights = np.array([1.4, 1.9, 3.2])\n",
    "    slope_fixed = 0.64\n",
    "    intercept = 0.0\n",
    "\n",
    "    print(f\"--- Mini-batch GD (Batch Size: {batch_size}) ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        infices = np.arange(len(weights))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        #vai criar as batches\n",
    "        for start_idx in range(0, len(weights), batch_size):\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "            #dados da batch\n",
    "            w_batch = weights[batch_indices]\n",
    "            h_batch = heights[batch_indices]\n",
    "\n",
    "            #predicao e derivada somada para o batch\n",
    "            pred_batch = intercept + slope_fixed * w_batch\n",
    "            residuals = h_batch - pred_batch\n",
    "            derivative = np.sum(-2 * residuals)\n",
    "\n",
    "            step_size = derivative * learning_rate\n",
    "\n",
    "            old_intercept = intercept\n",
    "            intercept = intercept - step_size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
