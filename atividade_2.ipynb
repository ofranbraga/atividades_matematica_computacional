{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8185f7a",
   "metadata": {},
   "source": [
    "# Atividade MC 2 - Implementação de Gradiente Descendente e Redes Neurais\n",
    "\n",
    "Bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Configuração para melhorar a visualização dos gráficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b6868",
   "metadata": {},
   "source": [
    "## Questão A: Gradiente Descendente (Regressão Linear)\n",
    "\n",
    "**Objetivo:** Implementar o procedimento dos slides 156-213.\n",
    "* **Dados:** Altura vs Peso.\n",
    "* **Modelo:** `Altura = Intercept + 0.64 * Peso` (O declive/slope é fixo em 0.64).\n",
    "* **Tarefa:** Otimizar o **Intercept** começando de 0.\n",
    "* **Visualização:** Mostrar o movimento da reta e a descida na curva de custo (SSR).\n",
    "* **Condições de parada:** Número máximo de iterações OU Step Size muito pequeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_question_a(learning_rate, max_iter=20, tolerance=0.001):\n",
    "    #dados do slide \n",
    "    weights = np.array([0.5, 2.3, 2.9])\n",
    "    heights = np.array([1.4, 1.9, 3.2])\n",
    "    \n",
    "    #parâmetros Iniciais que foram definidos no slide\n",
    "    slope_fixed = 0.64\n",
    "    intercept = 0.0  #começa em 0 como esta no slide\n",
    "    \n",
    "    #para plotagem da curva de custo\n",
    "    intercept_vals = np.linspace(-0.5, 1.5, 100)\n",
    "    ssr_vals = []\n",
    "    for i in intercept_vals:\n",
    "        predicted = i + slope_fixed * weights\n",
    "        residuals = heights - predicted\n",
    "        ssr_vals.append(np.sum(residuals**2))\n",
    "        \n",
    "    print(f\"\\n--- Iniciando Gradiente Descendente (LR: {learning_rate}) ---\")\n",
    "    \n",
    "    #loop para otimização do intercept\n",
    "    for i in range(max_iter):\n",
    "        #calcular Predições e Derivada\n",
    "        predicted_heights = intercept + slope_fixed * weights\n",
    "        \n",
    "        #calculo do predicted e da derivada\n",
    "        residuals = heights - predicted_heights\n",
    "        derivative = np.sum(-2 * residuals)\n",
    "        \n",
    "        # calcular Step Size\n",
    "        step_size = derivative * learning_rate\n",
    "        \n",
    "        #guardar valor antigo para impressão\n",
    "        old_intercept = intercept\n",
    "        \n",
    "        #atualizar intercept\n",
    "        new_intercept = old_intercept - step_size\n",
    "        \n",
    "        print(f\"Iter {i+1}: Old Intercept={old_intercept:.3f}, Step Size={step_size:.3f}, New Intercept={new_intercept:.3f}, Slope(Grad)={derivative:.3f}\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        #grafico da reta de regressão\n",
    "        axes[0].scatter(weights, heights, color='green', s=100, label='Dados Reais')\n",
    "        axes[0].plot(weights, predicted_heights, color='teal', linewidth=2, label=f'Reta (Intercept={old_intercept:.2f})')\n",
    "\n",
    "        for w, h, p in zip(weights, heights, predicted_heights):\n",
    "            axes[0].plot([w, w], [h, p], color='red', linestyle='--')\n",
    "        axes[0].set_title(f\"Iteração {i+1}: Ajuste da Reta\")\n",
    "        axes[0].set_xlabel(\"Peso\")\n",
    "        axes[0].set_ylabel(\"Altura\")\n",
    "        axes[0].legend()\n",
    "        axes[0].set_ylim(0, 4)\n",
    "        \n",
    "        # Gráfico da curva de custo\n",
    "        current_ssr = np.sum((heights - (old_intercept + slope_fixed * weights))**2)\n",
    "        axes[1].plot(intercept_vals, ssr_vals, color='teal')\n",
    "        axes[1].scatter(old_intercept, current_ssr, color='red', s=100, zorder=5)\n",
    "        #tangente (visualização do gradiente)\n",
    "        tangent_line = derivative * (intercept_vals - old_intercept) + current_ssr\n",
    "        axes[1].plot(intercept_vals, tangent_line, color='orange', linestyle='--', alpha=0.5, label='Inclinação (Derivada)')\n",
    "        \n",
    "        axes[1].set_title(f\"Soma dos Resíduos ao Quadrado vs Intercept\")\n",
    "        axes[1].set_xlabel(\"Intercept\")\n",
    "        axes[1].set_ylabel(\"SSR\")\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        intercept = new_intercept\n",
    "        \n",
    "        #condição de parada 2: step size muito pequeno\n",
    "        if abs(step_size) < tolerance:\n",
    "            print(\"-> Convergência alcançada (Step Size pequeno).\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Resultado Final para LR {learning_rate}: Intercept = {intercept:.4f}\")\n",
    "\n",
    "\n",
    "run_question_a(learning_rate=0.1)\n",
    "\n",
    "run_question_a(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b77a2",
   "metadata": {},
   "source": [
    "## Questão B: Gradiente Descendente Estocástico e Mini-Batch\n",
    "\n",
    "**Objetivo:** Transformar o procedimento dos slides 302-335 em código.\n",
    "* **Diferença:** Não atualizar usando a soma de *todos* os dados de uma vez, mas sim amostra por amostra (Estocástico) ou em pequenos grupos (Mini-batch).\n",
    "* **Nota:** O gráfico complexo da Questão A não será gerado aqui, focaremos na convergência."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a04927",
   "metadata": {},
   "source": [
    "### B-1: Gradiente Descendente Estocástico (SGD)\n",
    "Atualiza o intercept após calcular o erro para **cada** amostra individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d64cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sgd(learning_rate=0.1, epochs=5):\n",
    "    weights = np.array([0.5, 2.3, 2.9])\n",
    "    heights = np.array([1.4, 1.9, 3.2])\n",
    "    \n",
    "    slope_fixed = 0.64\n",
    "    intercept = 0.0\n",
    "    \n",
    "    intercept_vals = np.linspace(-0.5, 1.5, 100)\n",
    "    ssr_vals = []\n",
    "    for i in intercept_vals:\n",
    "        predicted = i + slope_fixed * weights\n",
    "        residuals = heights - predicted\n",
    "        ssr_vals.append(np.sum(residuals**2))\n",
    "\n",
    "    print(f\"--- SGD com Gráficos (Learning Rate: {learning_rate}) ---\")\n",
    "\n",
    "    step_count = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(len(weights))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        \n",
    "        for i in indices:\n",
    "            #vai selecionar apenas uma amostra\n",
    "            w_sample = weights[i]\n",
    "            h_sample = heights[i]\n",
    "            \n",
    "            pred_sample = intercept + slope_fixed * w_sample\n",
    "\n",
    "            derivative = -2 * (h_sample - pred_sample)\n",
    "\n",
    "            step_size = derivative * learning_rate\n",
    "            \n",
    "            old_intercept = intercept\n",
    "            intercept = intercept - step_size\n",
    "           \n",
    "            #VISUALIZACAO\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "            #grafico da reta de regressão\n",
    "            axes[0].scatter(weights, heights, color='green', s=100, alpha=0.3, label='Outros Dados')#reta de regressão\n",
    "            axes[0].scatter([w_sample], [h_sample], color='red', s=150, edgecolors='black', label='Amostra Atual (SGD)')#destaca o ponto atual usado no SGD\n",
    "\n",
    "            #rreta atual\n",
    "            predicted_all = intercept + slope_fixed * weights\n",
    "            axes[0].plot(weights, predicted_all, color='teal', linewidth=2, label=f'Reta (Intercept={intercept:.2f})')\n",
    "\n",
    "            axes[0].set_title(f\"Passo {step_count} (Epoch {epoch+1}): Ajuste pela Amostra Individual\")\n",
    "            axes[0].set_xlabel(\"Peso\")\n",
    "            axes[0].set_ylabel(\"Altura\")\n",
    "            axes[0].legend()\n",
    "            axes[0].set_ylim(0, 4)\n",
    "\n",
    "            #gráfico da curva de custo\n",
    "            current_total_ssr = np.sum((heights - (intercept + slope_fixed * weights))**2)\n",
    "\n",
    "            axes[1].plot(intercept_vals, ssr_vals, color='teal', alpha=0.6)\n",
    "            axes[1].scatter(intercept, current_total_ssr, color='red', s=100, zorder=5)\n",
    "\n",
    "            axes[1].set_title(f\"Trajetória na SSR Global\")\n",
    "            axes[1].set_xlabel(\"Intercept\")\n",
    "            axes[1].set_ylabel(\"Soma dos Resíduos ao Quadrado (SSR)\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"Amostra (w={w_sample}): Old={old_intercept:.3f}, Step={step_size:.3f}, New={intercept:.3f}\")\n",
    "\n",
    "        return intercept\n",
    "\n",
    "final_intercept_sgd = run_sgd()\n",
    "print(f\"Final SGD Intercept= {final_intercept_sgd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0feff5d",
   "metadata": {},
   "source": [
    "### B-2: Gradiente Descendente Mini-Batch\n",
    "Adaptação para usar **mini-batch de 2 samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42333ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minibatch(learning_rate=0.1, epochs=5, batch_size=2):\n",
    "    weights = np.array([0.5, 2.3, 2.9])\n",
    "    heights = np.array([1.4, 1.9, 3.2])\n",
    "    \n",
    "    slope_fixed = 0.64\n",
    "    intercept = 0.0\n",
    "\n",
    "    intercept_vals = np.linspace(-0.5, 1.5, 100)\n",
    "    ssr_vals = []\n",
    "    for i in intercept_vals:\n",
    "        predicted = i + slope_fixed * weights\n",
    "        residuals_curve = heights - predicted\n",
    "        ssr_vals.append(np.sum(residuals_curve**2))\n",
    "\n",
    "    print(f\"---Mini-batch GD com Gráficos  (Batch Size: {batch_size}) ---\")\n",
    "\n",
    "    step_count = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #embaralhar os dados\n",
    "        indices = np.arange(len(weights))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        #vai criar as batches\n",
    "        for start_idx in range(0, len(weights), batch_size):\n",
    "            step_count += 1\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "            #dados da batch\n",
    "            w_batch = weights[batch_indices]\n",
    "            h_batch = heights[batch_indices]\n",
    "\n",
    "            #predicao e derivada somada para o batch\n",
    "            pred_batch = intercept + slope_fixed * w_batch\n",
    "            residuals = h_batch - pred_batch\n",
    "            derivative = np.sum(-2 * residuals)\n",
    "\n",
    "            step_size = derivative * learning_rate\n",
    "\n",
    "            old_intercept = intercept\n",
    "            intercept = intercept - step_size\n",
    "\n",
    "            #VISUALIZACAO\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "            #Ggrafico 1\n",
    "            axes[0].scatter(weights, heights, color='green', s=100, alpha=0.3, label='Outros Dados')#reta de regressão\n",
    "            axes[0].scatter(w_batch, h_batch, color='orange', s=150, edgecolors='black', label=f'Batch Atual (n={len(w_batch)})')#destaca o ponto\n",
    "\n",
    "            predicted_all = intercept + slope_fixed * weights\n",
    "            axes[0].plot(weights, predicted_all, color='teal', linewidth=2, label=f'Reta (Intercept={intercept:.2f})')\n",
    "\n",
    "            axes[0].set_title(f\"Passo {step_count} (Época {epoch+1}): Ajuste pelo Mini-Batch\")\n",
    "            axes[0].set_xlabel(\"Peso\")\n",
    "            axes[0].set_ylabel(\"Altura\")\n",
    "            axes[0].legend()\n",
    "            axes[0].set_ylim(0, 4)\n",
    "\n",
    "            #grafico 2\n",
    "            current_total_ssr = np.sum((heights - (intercept + slope_fixed * weights))**2)\n",
    "            axes[1].plot(intercept_vals, ssr_vals, color='teal', alpha=0.6)\n",
    "            axes[1].scatter(intercept, current_total_ssr, color='red', s=100, zorder=5)\n",
    "\n",
    "            axes[1].set_title(f\"Trajetória na SSR Global\")\n",
    "            axes[1].set_xlabel(\"Intercept\")\n",
    "            axes[1].set_ylabel(\"SSR\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"Batch: Old={old_intercept:.3f}, Step={step_size:.3f}, New={intercept:.3f}\")\n",
    "        \n",
    "        return intercept\n",
    "\n",
    "final_intercept_batch = run_minibatch()\n",
    "print(f\"Final Mini-Batch Intercept: {final_intercept_batch:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22978152",
   "metadata": {},
   "source": [
    "## Questão D: Gradiente Descendente em Redes Neurais (Foco em $b_3$)\n",
    "\n",
    "**Objetivo:** Reproduzir a demonstração dos slides 256-305.\n",
    "* **Rede Neural:** Input -> Hidden Layer (2 nós, Softplus) -> Output.\n",
    "* **Parâmetro a otimizar:** Apenas o viés (bias) **$b_3$**.\n",
    "* **Função de Ativação:** Softplus `log(1 + e^x)`.\n",
    "* **Dados:**\n",
    "    * Dosage (Input): [0, 0.5, 1]\n",
    "    * Observed (Target): [0, 1, 0]\n",
    "* **Valores Fixos (dos slides):**\n",
    "    * $w_1 = 3.34, b_1 = -1.43$\n",
    "    * $w_2 = -3.53, b_2 = 0.57$\n",
    "    * $w_3 = -1.22, w_4 = -2.30$ (Valores observados nos slides para o exemplo de otimização de b3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc79513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def run_nn_b3_optimization(learning_rate=0.1, max_iter=100, tolerance=0.001):\n",
    "    #dados\n",
    "    inputs = np.array([0, 0.5, 1])\n",
    "    observed = np.array([0, 1, 0])\n",
    "    \n",
    "    #pesos fixos\n",
    "    w1, b1 = 3.34, -1.43\n",
    "    w2, b2 = -3.53, 0.57\n",
    "    w3, w4 = -1.22, -2.30 \n",
    "    \n",
    "    #parâmetro a otimizar\n",
    "    b3 = 0.0 #inicialização\n",
    "    \n",
    "    print(f\"\\n--- Otimizando b3 na Rede Neural (LR: {learning_rate}) ---\")\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # CORREÇÃO: Inicializar a soma das derivadas antes de percorrer os dados\n",
    "        total_derivative_b3 = 0.0\n",
    "        \n",
    "        # Loop pelos dados para calcular o gradiente total (Soma das derivadas)\n",
    "        for j in range(len(inputs)):\n",
    "            input_val = inputs[j]\n",
    "            obs_val = observed[j]\n",
    "            \n",
    "            #neurônio 1 \n",
    "            x1 = input_val * w1 + b1\n",
    "            y1 = softplus(x1)\n",
    "            \n",
    "            #neurônio 2\n",
    "            x2 = input_val * w2 + b2\n",
    "            y2 = softplus(x2)\n",
    "            \n",
    "            predicted = (y1 * w3) + (y2 * w4) + b3\n",
    "            \n",
    "            #backpropagation para b3\n",
    "            deriv = -2 * (obs_val - predicted) * 1\n",
    "            \n",
    "            # Acumular a derivada\n",
    "            total_derivative_b3 += deriv\n",
    "            \n",
    "        step_size = total_derivative_b3 * learning_rate\n",
    "        \n",
    "        old_b3 = b3\n",
    "        \n",
    "        b3 = old_b3 - step_size\n",
    "        \n",
    "        print(f\"Iter {i+1}: Old b3={old_b3:.3f}, Step Size={step_size:.3f}, New b3={b3:.3f}\")\n",
    "        \n",
    "        #condição de parada por tolerância (se o passo for muito pequeno)\n",
    "        if abs(step_size) < tolerance:\n",
    "            print(f\"-> Convergência alcançada em {i+1} iterações.\")\n",
    "            break\n",
    "            \n",
    "    return b3\n",
    "\n",
    "final_b3 = run_nn_b3_optimization()\n",
    "print(f\"Valor final de b3: {final_b3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44bfb7",
   "metadata": {},
   "source": [
    "## Questão E: Otimizando 3 Parâmetros ($w_3, w_4, b_3$)\n",
    "\n",
    "**Objetivo:** Reproduzir slides 231-279.\n",
    "* Agora otimizamos os pesos da conexão da camada oculta para a saída e o bias de saída simultaneamente.\n",
    "* **Inicialização:** Aleatória para pesos, 0 para bias (conforme slide 231)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f83a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def run_nn_3param_optmization(learning_rate=0.1, max_iter=1000, tolerance=0.0001):\n",
    "    inputs = np.array([0, 0.5, 1])\n",
    "    observed = np.array([0, 1, 0])\n",
    "    \n",
    "    #pesos fixos\n",
    "    w1, b1 = 3.34, -1.43\n",
    "    w2, b2 = -3.53, 0.57\n",
    "\n",
    "    #parâmetros a otimizar\n",
    "    np.random.seed(42)\n",
    "    w3 = np.random.randn()\n",
    "    w4 = np.random.randn()\n",
    "    b3 = 0.0 \n",
    "\n",
    "    print(f\"\\n --- Otimizando w3, w4, b3 simultaneamente ---\")\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        grad_w3 = 0.0\n",
    "        grad_w4 = 0.0\n",
    "        grad_b3 = 0.0\n",
    "\n",
    "        for j in range(len(inputs)):\n",
    "            input_val = inputs[j]\n",
    "            obs_val = observed[j]\n",
    "\n",
    "            y1 = softplus(input_val * w1 + b1) #ativa no superior \n",
    "            y2 = softplus(input_val * w2 + b2) #ativa o no inferior\n",
    "\n",
    "            predicted = (y1 * w3) + (y2 * w4) + b3\n",
    "\n",
    "            #parte comum da derivada (chain rule)\n",
    "            base_deriv = -2 * (obs_val - predicted)\n",
    "\n",
    "            #gradienetes especificos\n",
    "\n",
    "            grad_w3 += base_deriv * y1 #derivada em relação a w3\n",
    "            grad_w4 += base_deriv * y2 #derivada em relação a w4\n",
    "            grad_b3 += base_deriv * 1 #derivada em relação a b3\n",
    "\n",
    "        #calcula os step sizes\n",
    "        step_w3 = grad_w3 * learning_rate\n",
    "        step_w4 = grad_w4 * learning_rate\n",
    "        step_b3 = grad_b3 * learning_rate\n",
    "\n",
    "        #atualiza\n",
    "        old_w3, old_w4, old_b3 = w3, w4, b3\n",
    "\n",
    "        w3 = w3 - step_w3\n",
    "        w4 = w4 - step_w4\n",
    "        b3 = b3 - step_b3\n",
    "\n",
    "        #print a cada 20 iterações\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Iter {i}:\")\n",
    "            print(f\"   w3: Old={old_w3:.3f}, Step={step_w3:.3f}, New={w3:.3f}\")\n",
    "            print(f\"   w4: Old={old_w4:.3f}, Step={step_w4:.3f}, New={w4:.3f}\")\n",
    "            print(f\"   b3: Old={old_b3:.3f}, Step={step_b3:.3f}, New={b3:.3f}\")\n",
    "\n",
    "        #condição de parada por tolerância\n",
    "        if abs(step_w3) < tolerance and abs(step_w4) < tolerance and abs(step_b3) < tolerance:\n",
    "            print(f\"-> Convergência alcançada na iteração {i}.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nValores Finais: w3={w3:.3f}, w4={w4:.3f}, b3={b3:.3f}\")\n",
    "\n",
    "run_nn_3param_optmization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
